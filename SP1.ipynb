{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981014e6-9e93-4bed-b16c-738b352e99ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7bd871-f015-4be7-af5e-b605b1666ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"EdinburghNLP/xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55e77a3-69e8-4cb8-aa41-af1084703cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary', 'id'],\n",
      "    num_rows: 1000\n",
      "}) Dataset({\n",
      "    features: ['document', 'summary', 'id'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define sample sizes\n",
    "train_sample_size = 1000  # Adjust as needed\n",
    "valid_sample_size = 200\n",
    "\n",
    "# Randomly shuffle and select\n",
    "train_subset = dataset[\"train\"].shuffle(seed=42).select(range(train_sample_size))\n",
    "valid_subset = dataset[\"validation\"].shuffle(seed=42).select(range(valid_sample_size))\n",
    "\n",
    "print(train_subset, valid_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2b96cf-680f-47fa-a242-fd03950654d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e21072a49d41b4aeb69b34925dfe15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a817aa6d3af4578af520939373f3d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"facebook/bart-large-cnn\"  # Change this for other models like t5-small or google/pegasus-xsum\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"document\"]\n",
    "    targets = examples[\"summary\"]\n",
    "    \n",
    "    # Tokenize inputs and summaries\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=70, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_subset.map(preprocess_function, batched=True)\n",
    "tokenized_valid = valid_subset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc763d4b-b86e-4c49-b8b2-4a3b74598b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_sample_size = 100\n",
    "test_subset = dataset[\"test\"].shuffle(seed=42).select(range(test_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce857bc9-7627-4320-8671-3ef63a726e76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae11ce0e-093a-4750-8626-7f9a43b730e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0dbd970-2542-4bb5-8703-bef03555dbfe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 05:55:31.800709: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-01 05:55:31.800709: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-01 05:55:31.801768: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 05:55:31.812161: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-01 05:55:33.481664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 22:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.030000</td>\n",
       "      <td>0.837885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422700</td>\n",
       "      <td>0.891701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>1.094819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.5423719635009766, metrics={'train_runtime': 1375.4226, 'train_samples_per_second': 2.181, 'train_steps_per_second': 1.091, 'total_flos': 6501313806336000.0, 'train_loss': 0.5423719635009766, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e96af434-e372-4fee-9e59-7f93bf9d2535",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
      "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.1.1+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.0)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.35.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.66.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (23.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/lib/python3/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.5)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.20.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (15.0.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets>=2.0.0->evaluate) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.0.1->bert_score) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2023.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->bert_score) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->bert_score) (2020.6.20)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.3)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->bert_score) (2.4.7)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bert_score rouge_score evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d32d4bea-7368-4b87-a426-6ed7655056c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb63981508744ab094f868d25bf0c025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE Scores: {'rouge1_f1': 0.27812040419995043, 'rouge2_f1': 0.09386065001259182, 'rougeL_f1': 0.19941973693805942}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BERTScore: {'precision': 0.8635722690820694, 'recall': 0.8970527470111846, 'f1': 0.8798836487531662}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import evaluate\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results/checkpoint-1000\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"results/checkpoint-1000\").to(\"cuda\").half()\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "def generate_answer(batch):\n",
    "    # Tokenize the inputs with truncation\n",
    "    inputs = tokenizer(batch[\"document\"], padding=\"max_length\", max_length=1024, return_tensors=\"pt\", truncation=True)\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        predicted_ids = model.generate(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    batch[\"predicted_summary\"] = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return batch\n",
    "\n",
    "# Apply summarization function\n",
    "test_data = test_subset.map(generate_answer, batched=True, batch_size=4)\n",
    "\n",
    "# Load ROUGE metric\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Compute ROUGE score for the entire dataset\n",
    "rouge_scores = rouge.compute(predictions=test_data[\"predicted_summary\"], references=test_subset[\"summary\"])\n",
    "\n",
    "\n",
    "# Calculate the average ROUGE scores (F1-score, precision, recall)\n",
    "avg_rouge = {\n",
    "    \"rouge1_f1\": rouge_scores[\"rouge1\"],\n",
    "    \"rouge2_f1\": rouge_scores[\"rouge2\"],\n",
    "    \"rougeL_f1\": rouge_scores[\"rougeL\"]\n",
    "}\n",
    "\n",
    "print(f\"Average ROUGE Scores: {avg_rouge}\")\n",
    "\n",
    "# Now let's calculate BERTScore\n",
    "\n",
    "# Load BERTScore metric\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Compute BERTScore for the entire dataset\n",
    "bertscore_results = bertscore.compute(predictions=test_data[\"predicted_summary\"], references=test_subset[\"summary\"], lang=\"en\")\n",
    "\n",
    "\n",
    "# Calculate the average BERTScore (F1-score, precision, recall)\n",
    "avg_bertscore = {\n",
    "    \"precision\": sum(bertscore_results[\"precision\"]) / len(bertscore_results[\"precision\"]),\n",
    "    \"recall\": sum(bertscore_results[\"recall\"]) / len(bertscore_results[\"recall\"]),\n",
    "    \"f1\": sum(bertscore_results[\"f1\"]) / len(bertscore_results[\"f1\"])\n",
    "}\n",
    "\n",
    "print(f\"Average BERTScore: {avg_bertscore}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e278782b-3c0a-4c2b-b909-35e5a20578cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "Actual Summary: A woman who was seriously hurt in a fatal hen party motorway crash is now helping other major trauma victims rebuild their lives.\n",
      "Predicted Summary: A woman who was seriously injured in a crash which killed her friend has spoken publicly for the first time since the incident to thank a charity for helping her cope with the trauma caused by a fatal crash in which left her \"giddy and giddy\" on the way to a hen party.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example 2:\n",
      "Actual Summary: A Tudor manor house has reopened following a £2.2m makeover.\n",
      "Predicted Summary: A £1.6m renovation of a 19th Century timber-framed hall known as the \"jewel in the crown\" of a family estate has opened to the public after months of work by the council and the Heritage Lottery Fund, it has been announced.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example 3:\n",
      "Actual Summary: Walt Disney World has unveiled a lighthouse memorial for a young boy who was killed by an alligator while on holiday at the Florida theme park.\n",
      "Predicted Summary: A sculpture to commemorate a boy killed in an alligator attack at Walt Disney World has been installed at the site of the boy's death in which a lighthouse sculpture was built to commemorate the boy, who was dragged underwater by an animal while collecting water for a sandcastle.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example 4:\n",
      "Actual Summary: The agriculture minister is to return to China next month as attempts to access markets there continue.\n",
      "Predicted Summary: Northern Ireland's agriculture minister has said she wants to use the country's \"reputation around the world\" to help to open doors to Chinese and US markets as part of a trade deal to boost the local meat industry.Michelle O'Neill said she was working to resolve labelling issues that have hit local lamb sales to the Republic of Ireland.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Example 5:\n",
      "Actual Summary: Blackpool have signed Bournemouth goalkeeper Ryan Allsop and midfielder Callum Cooke from Middlesbrough on season-long loans.\n",
      "Predicted Summary: Blackpool have signed Premier League midfielder Joe Allsop on a season-long loan from League Two club Crewe Alexandra for a club-record fee of £1.5m. Blackpool have also signed England Under-17 international Luke Cooke on a three-year deal.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Actual Summary: {test_data['summary'][i]}\")\n",
    "    print(f\"Predicted Summary: {test_data['predicted_summary'][i]}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a8d8cbc-ceb4-472e-ae56-f9d2f9e75d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'summaries_comparison.csv' has been saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare lists to store data\n",
    "actual_summaries = []\n",
    "predicted_summaries = []\n",
    "\n",
    "# Collect summaries into the lists\n",
    "for i in range(100):\n",
    "    actual_summaries.append(test_data['summary'][i])\n",
    "    predicted_summaries.append(test_data['predicted_summary'][i])\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Actual Summary': actual_summaries,\n",
    "    'Predicted Summary': predicted_summaries\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"summaries_comparison.csv\", index=False)\n",
    "\n",
    "print(\"CSV file 'summaries_comparison.csv' has been saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
