{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InLFsu_NGvmV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets soundfile accelerate speechbrain==0.5.16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4YEzYfYGxQf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJdb1_5gHN_M",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Audio\n",
        "dataset = load_dataset(\"keithito/lj_speech\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33GgyjE3HtFX",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset = dataset[\"train\"]\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaI2t0mZKbVp",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Calculate the number of rows for half the dataset\n",
        "half_size = len(dataset) //5\n",
        "\n",
        "# Select the first half of the dataset\n",
        "dataset = dataset.select(range(half_size))\n",
        "\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK4qrOobKlTF"
      },
      "source": [
        "We are using just the 1/16th of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKXQnTfoJNHe",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZaO2dNGJO6r",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import SpeechT5Processor\n",
        "\n",
        "checkpoint = \"microsoft/speecht5_tts\"\n",
        "processor = SpeechT5Processor.from_pretrained(checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41HXlo_WJQZ8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tokenizer = processor.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuJDX9PxpkOD",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install librosa soundfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgVKf7yEJR3Y",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset[2:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0GPu2dUHSfE"
      },
      "source": [
        "Let's normalize the dataset, create a column called \"normalized_text\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUDgWGNbJTJl",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def extract_all_chars(batch):\n",
        "    all_text = \" \".join(batch[\"text\"])\n",
        "    vocab = list(set(all_text))\n",
        "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "\n",
        "vocabs = dataset.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
        "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWPcRXZpJY-W",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset_vocab - tokenizer_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AfJj2FNJtKQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def extract_all_chars(batch):\n",
        "    all_text = \" \".join(batch[\"normalized_text\"])\n",
        "    vocab = list(set(all_text))\n",
        "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "\n",
        "vocabs = dataset.map(\n",
        "    extract_all_chars,\n",
        "    batched=True,\n",
        "    batch_size=-1,\n",
        "    keep_in_memory=True,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
        "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJRpRh-JbjXZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset_vocab - tokenizer_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRzENvUhbn-J",
        "tags": []
      },
      "outputs": [],
      "source": [
        "replacements = [\n",
        "    (\"â\", \"a\"),  # Long a\n",
        "    (\"ç\", \"ch\"),  # Ch as in \"chair\"\n",
        "    (\"ğ\", \"gh\"),  # Silent g or slight elongation of the preceding vowel\n",
        "    (\"ı\", \"i\"),   # Dotless i\n",
        "    (\"î\", \"i\"),   # Long i\n",
        "    (\"ö\", \"oe\"),  # Similar to German ö\n",
        "    (\"ş\", \"sh\"),  # Sh as in \"shoe\"\n",
        "    (\"ü\", \"ue\"),  # Similar to German ü\n",
        "    (\"û\", \"u\"),   # Long u\n",
        "]\n",
        "\n",
        "def cleanup_text(inputs):\n",
        "    for src, dst in replacements:\n",
        "        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
        "    return inputs\n",
        "\n",
        "dataset = dataset.map(cleanup_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ6mMZNbdLfI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "speaker_model = EncoderClassifier.from_hparams(\n",
        "    source=spk_model_name,\n",
        "    run_opts={\"device\": device},\n",
        "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
        ")\n",
        "\n",
        "\n",
        "def create_speaker_embedding(waveform):\n",
        "    with torch.no_grad():\n",
        "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
        "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
        "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
        "    return speaker_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdM2RWNfg1gH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        text=example[\"normalized_text\"],\n",
        "        audio_target=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "    # strip off the batch dimension\n",
        "    example[\"labels\"] = example[\"labels\"][0]\n",
        "\n",
        "    # use SpeechBrain to obtain x-vector\n",
        "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JI6x3PD-g13d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "processed_example = prepare_dataset(dataset[0])\n",
        "list(processed_example.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oTcqW8sg3os",
        "tags": []
      },
      "outputs": [],
      "source": [
        "processed_example[\"speaker_embeddings\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpF2x6jlg8-U",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnoDYrafhBeP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def is_not_too_long(input_ids):\n",
        "    input_length = len(input_ids)\n",
        "    return input_length < 200\n",
        "\n",
        "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux7w-nybhNCF",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_test = dataset.train_test_split(test_size=0.3)  # 70% train, 30% temp\n",
        "val_test = train_test[\"test\"].train_test_split(test_size=0.5)  # Split temp into 15% val, 15% test\n",
        "\n",
        "dataset = {\n",
        "    \"train\": train_test[\"train\"],\n",
        "    \"validation\": val_test[\"train\"],\n",
        "    \"test\": val_test[\"test\"]\n",
        "}\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXRcMXXdhQ1C",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TTSDataCollatorWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
        "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
        "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
        "\n",
        "        # collate the inputs and targets into a batch\n",
        "        batch = processor.pad(\n",
        "            input_ids=input_ids, labels=label_features, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        batch[\"labels\"] = batch[\"labels\"].masked_fill(\n",
        "            batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100\n",
        "        )\n",
        "\n",
        "        # not used during fine-tuning\n",
        "        del batch[\"decoder_attention_mask\"]\n",
        "\n",
        "        # round down target lengths to multiple of reduction factor\n",
        "        if model.config.reduction_factor > 1:\n",
        "            target_lengths = torch.tensor(\n",
        "                [len(feature[\"input_values\"]) for feature in label_features]\n",
        "            )\n",
        "            target_lengths = target_lengths.new(\n",
        "                [\n",
        "                    length - length % model.config.reduction_factor\n",
        "                    for length in target_lengths\n",
        "                ]\n",
        "            )\n",
        "            max_length = max(target_lengths)\n",
        "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
        "\n",
        "        # also add in the speaker embeddings\n",
        "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z2Q_KrshSNi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data_collator = TTSDataCollatorWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXVZzQYphZim",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import SpeechT5ForTextToSpeech\n",
        "\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESMgm4K5hdDi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "# disable cache during training since it's incompatible with gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# set language and task for generation and re-enable cache\n",
        "model.generate = partial(model.generate, use_cache=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8BoZ8xuhe8i",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"speecht5_finetuned_madhav\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=100,\n",
        "    max_steps=229,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    save_steps=100,\n",
        "    eval_steps=100,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    greater_is_better=False,\n",
        "    label_names=[\"labels\"],\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZqNCuuapkOI",
        "tags": []
      },
      "outputs": [],
      "source": [
        "num_epochs = 4  # Change this as needed\n",
        "dataset_size = len(dataset[\"train\"])  # Get number of training samples\n",
        "effective_batch_size = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
        "\n",
        "max_steps = (num_epochs * dataset_size) // effective_batch_size\n",
        "print(f\"Calculated max_steps: {max_steps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-HhrQ_Gh2Gf",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_test[\"train\"],\n",
        "    eval_dataset=val_test[\"train\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Le1psSjbXv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEBxmn6LyM_E"
      },
      "outputs": [],
      "source": [
        "#Load Dataset from scratch for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpN_Yu-lw3Rm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from transformers import SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        "from pesq import pesq  # Make sure to install pypesq for PESQ calculation\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# Load the models\n",
        "tts_model = SpeechT5ForTextToSpeech.from_pretrained(\"speecht5_finetuned_madhav/checkpoint-229\")\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "# Transcription function for WER computation using Wav2Vec2\n",
        "def transcribe_audio(file_path):\n",
        "    from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "    processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h-lv60-self\")\n",
        "\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "\n",
        "    # Resample if the sample rate is not 16000\n",
        "    if sample_rate != 16000:\n",
        "        resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Transcribe the audio\n",
        "    inputs = processor(waveform.squeeze().numpy(), return_tensors=\"pt\", sampling_rate=16000)\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_values=inputs.input_values).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)\n",
        "\n",
        "    return transcription[0]\n",
        "\n",
        "# Function to compute WER (Word Error Rate) manually\n",
        "def compute_wer(reference_audio_path, predicted_audio_path):\n",
        "    reference_transcription = transcribe_audio(reference_audio_path)\n",
        "    predicted_transcription = transcribe_audio(predicted_audio_path)\n",
        "\n",
        "    # Compute WER manually (Levenshtein distance)\n",
        "    reference_words = reference_transcription.split()\n",
        "    predicted_words = predicted_transcription.split()\n",
        "\n",
        "    distance = levenshtein_distance(reference_words, predicted_words)\n",
        "    wer_score = distance / len(reference_words)\n",
        "    return wer_score\n",
        "\n",
        "# Levenshtein distance for computing WER\n",
        "def levenshtein_distance(ref, pred):\n",
        "    m = len(ref) + 1\n",
        "    n = len(pred) + 1\n",
        "    matrix = np.zeros((m, n))\n",
        "    for i in range(m):\n",
        "        matrix[i][0] = i\n",
        "    for j in range(n):\n",
        "        matrix[0][j] = j\n",
        "    for i in range(1, m):\n",
        "        for j in range(1, n):\n",
        "            cost = 0 if ref[i-1] == pred[j-1] else 1\n",
        "            matrix[i][j] = min(matrix[i-1][j] + 1,\n",
        "                               matrix[i][j-1] + 1,\n",
        "                               matrix[i-1][j-1] + cost)\n",
        "    return matrix[m-1][n-1]\n",
        "\n",
        "# Function to compute Mel Cepstral Distortion (MCD)\n",
        "import librosa.util\n",
        "\n",
        "def compute_mcd(ref_wav, pred_wav):\n",
        "    ref, _ = librosa.load(ref_wav, sr=16000)\n",
        "    pred, _ = librosa.load(pred_wav, sr=16000)\n",
        "\n",
        "    mel_ref = librosa.feature.melspectrogram(y=ref, sr=16000, n_mels=23)\n",
        "    mel_pred = librosa.feature.melspectrogram(y=pred, sr=16000, n_mels=23)\n",
        "\n",
        "    # Align shapes by trimming or padding\n",
        "    min_frames = min(mel_ref.shape[1], mel_pred.shape[1])\n",
        "    mel_ref = mel_ref[:, :min_frames]\n",
        "    mel_pred = mel_pred[:, :min_frames]\n",
        "\n",
        "    # Compute MCD\n",
        "    mcd = np.sqrt(np.sum((mel_ref - mel_pred) ** 2) / mel_ref.size)\n",
        "    return mcd\n",
        "\n",
        "# Function to compute PESQ score\n",
        "def compute_pesq(ref_wav, pred_wav):\n",
        "    ref, _ = librosa.load(ref_wav, sr=16000, mono=True)\n",
        "    pred, _ = librosa.load(pred_wav, sr=16000, mono=True)\n",
        "\n",
        "    # PESQ requires float32 numpy arrays\n",
        "    ref = ref.astype(np.float32)\n",
        "    pred = pred.astype(np.float32)\n",
        "\n",
        "    # Use narrowband PESQ (for 8kHz) or wideband PESQ (for 16kHz)\n",
        "    score = pesq(16000, ref, pred, 'wb')  # 'wb' = Wideband PESQ for 16kHz audio\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "from transformers import SpeechT5Processor\n",
        "\n",
        "# Load processor (same one used in training)\n",
        "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
        "\n",
        "def evaluate_tts_model(test_set):\n",
        "    total_wer = 0\n",
        "    total_mcd = 0\n",
        "    total_pesq = 0\n",
        "    num_samples = len(test_set)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for idx, sample in enumerate(test_set):\n",
        "        text_input = sample[\"normalized_text\"]\n",
        "        reference_audio_path = sample[\"file\"]  # Get path to reference .wav file\n",
        "\n",
        "        # Generate input_ids for the model\n",
        "        inputs = processor(text=text_input, return_tensors=\"pt\")\n",
        "\n",
        "        # Use SpeechBrain to compute speaker embeddings\n",
        "        speaker_embeddings = create_speaker_embedding(sample[\"audio\"][\"array\"])\n",
        "        speaker_embeddings = torch.tensor(speaker_embeddings).unsqueeze(0)  # Ensure correct shape\n",
        "\n",
        "        # Generate speech using the model\n",
        "        generated_speech = tts_model.generate_speech(\n",
        "            inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder\n",
        "        )\n",
        "\n",
        "        # Save generated speech\n",
        "        predicted_audio_path = f\"predicted_{sample['id']}.wav\"\n",
        "        sf.write(predicted_audio_path, generated_speech.numpy(), 16000)\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        wer_score = compute_wer(reference_audio_path, predicted_audio_path)\n",
        "        mcd_score = compute_mcd(reference_audio_path, predicted_audio_path)\n",
        "        pesq_score = compute_pesq(reference_audio_path, predicted_audio_path)\n",
        "\n",
        "        # Store results\n",
        "        results.append({\"ID\": sample[\"id\"], \"WER\": wer_score, \"MCD\": mcd_score, \"PESQ\": pesq_score})\n",
        "        total_wer += wer_score\n",
        "        total_mcd += mcd_score\n",
        "        total_pesq += pesq_score\n",
        "\n",
        "        print(f\"Processed sample {idx + 1}/{num_samples}\")\n",
        "\n",
        "    # Compute average scores\n",
        "    avg_wer = total_wer / num_samples\n",
        "    avg_mcd = total_mcd / num_samples\n",
        "    avg_pesq = total_pesq / num_samples\n",
        "\n",
        "    print(f\"Average WER: {avg_wer:.4f}\")\n",
        "    print(f\"Average MCD: {avg_mcd:.4f}\")\n",
        "    print(f\"Average PESQ: {avg_pesq:.4f}\")\n",
        "\n",
        "    return results, avg_wer, avg_mcd, avg_pesq\n",
        "test_set = list(val_test[\"test\"])[:10]\n",
        "results, avg_wer, avg_mcd, avg_pesq = evaluate_tts_model(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oVec-aUN2is"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DttRwG_Qh6jP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import SpeechT5ForTextToSpeech\n",
        "model = SpeechT5ForTextToSpeech.from_pretrained(\n",
        "    \"speecht5_finetuned_madhav/checkpoint-229\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5Iipatxh_8z",
        "tags": []
      },
      "outputs": [],
      "source": [
        "example = val_test[\"test\"][1]\n",
        "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu_9NGOyiCop",
        "tags": []
      },
      "outputs": [],
      "source": [
        "text = \"A team of scientists has sent a camera down a seam in the Pacific Ocean in an attempt to see what marine life there is in the deep Pacific Ocean - and so far they've seen fish, jellyfish, mud volcanoes and deep sea coral, among other interesting creatures.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQTOgkZ9xCIA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Dictionary for number-to-word conversion\n",
        "number_words = {\n",
        "    0: \"zero\", 1: \"one\", 2: \"two\", 3: \"three\", 4: \"four\", 5: \"five\", 6: \"six\", 7: \"seven\", 8: \"eight\", 9: \"nine\",\n",
        "    10: \"ten\", 11: \"eleven\", 12: \"twelve\", 13: \"thirteen\", 14: \"fourteen\", 15: \"fifteen\", 16: \"sixteen\",\n",
        "    17: \"seventeen\", 18: \"eighteen\", 19: \"nineteen\", 20: \"twenty\", 30: \"thirty\", 40: \"forty\", 50: \"fifty\",\n",
        "    60: \"sixty\", 70: \"seventy\", 80: \"eighty\", 90: \"ninety\", 100: \"hundred\", 1000: \"thousand\"\n",
        "}\n",
        "\n",
        "def number_to_words(number):\n",
        "    if number < 20:\n",
        "        return number_words[number]\n",
        "    elif number < 100:\n",
        "        tens, unit = divmod(number, 10)\n",
        "        return number_words[tens * 10] + (\"-\" + number_words[unit] if unit else \"\")\n",
        "    elif number < 1000:\n",
        "        hundreds, remainder = divmod(number, 100)\n",
        "        return (number_words[hundreds] + \" hundred\" if hundreds > 0 else \"\") + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
        "    elif number < 1000000:\n",
        "        thousands, remainder = divmod(number, 1000)\n",
        "        return number_to_words(thousands) + \" thousand\" + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
        "    elif number < 1000000000:\n",
        "        millions, remainder = divmod(number, 1000000)\n",
        "        return number_to_words(millions) + \" million\" + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
        "    elif number < 1000000000000:\n",
        "        billions, remainder = divmod(number, 1000000000)\n",
        "        return number_to_words(billions) + \" billion\" + (\" \" + number_to_words(remainder) if remainder else \"\")\n",
        "    else:\n",
        "        return str(number)\n",
        "\n",
        "def replace_numbers_with_words(text):\n",
        "    def replace(match):\n",
        "        number = int(match.group())\n",
        "        return number_to_words(number)\n",
        "\n",
        "    # Replace numbers with words\n",
        "    result = re.sub(r'\\b\\d+\\b', replace, text)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klTFqedtpkOP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation (except apostrophes)\n",
        "    text = re.sub(r'[^\\w\\s\\']', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE9SBjH2j4Vk",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Function to clean up text using the replacement pairs\n",
        "def cleanup_text(text):\n",
        "    for src, dst in replacements:\n",
        "        text = text.replace(src, dst)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W842Kdt8j61e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "converted_text = replace_numbers_with_words(text)\n",
        "cleaned_text = cleanup_text(converted_text)\n",
        "final_text = normalize_text(cleaned_text)\n",
        "final_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a-_9v6i1_pm"
      },
      "outputs": [],
      "source": [
        "def split_text(text, max_length=15):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), max_length):\n",
        "        chunk = \" \".join(words[i:i+max_length])\n",
        "        chunks.append(chunk)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "427k1nlViGDn",
        "tags": []
      },
      "outputs": [],
      "source": [
        "text_chunks = split_text(final_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "or8TQBPyiLiA",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import SpeechT5HifiGan\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "for part_idx, chunk in enumerate(text_chunks):\n",
        "    # Convert text to input tensors\n",
        "    inputs = processor(text=chunk, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate speech using the model and vocoder\n",
        "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
        "\n",
        "    # Save each chunk separately\n",
        "    filename = f'predicted_summary_part{part_idx}.wav'\n",
        "    sf.write(filename, speech.numpy(), 16000)\n",
        "\n",
        "    print(f\"Generated speech for part {part_idx}: {chunk[:30]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGF5TQ5m4Zwc"
      },
      "outputs": [],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S289pqndiNDW",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# Initialize an empty audio segment\n",
        "final_audio = AudioSegment.silent(duration=0)\n",
        "\n",
        "# Combine all generated speech parts\n",
        "for part_idx in range(len(text_chunks)):  # Number of parts generated\n",
        "    filename = f'predicted_summary_part{part_idx}.wav'\n",
        "    audio_segment = AudioSegment.from_wav(filename)\n",
        "    final_audio += audio_segment  # Append each part to the final output\n",
        "\n",
        "# Export the merged audio to a single .wav file\n",
        "final_audio.export(\"final_predicted_summary.wav\", format=\"wav\")\n",
        "\n",
        "print(\"Final combined speech saved as 'final_predicted_summary.wav'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ttd4gtoDpkOP"
      },
      "outputs": [],
      "source": [
        "!zip -r output.zip /content/speecht5_finetuned_madhav/checkpoint-229"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQ_CKVp15ETv"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"output.zip\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPywnfOBJTZl"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng31xsHAJYBS"
      },
      "outputs": [],
      "source": [
        "!mv output.zip /content/drive/MyDrive/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}